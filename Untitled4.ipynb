{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUvqlD5Ymtzu0i/0yGZhNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maliozdemir1/dsa210-project-impact-of-climate-on-gastronomic-diversity/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V_9LgFznUgM",
        "outputId": "385d52bd-f6a0-4e2f-9ba0-0a00dfbc5708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of province links found: 83\n",
            "First 20: [('BAKU', 'Bakü Tahmini'), ('SARAYBOSNA', 'Saraybosna Tahmini'), ('ADANA', 'Adana'), ('ADIYAMAN', 'Adıyaman'), ('AFYONKARAHISAR', 'Afyonkarahisar'), ('AGRI', 'Ağrı'), ('AKSARAY', 'Aksaray'), ('AMASYA', 'Amasya'), ('ANKARA', 'Ankara'), ('ANTALYA', 'Antalya'), ('ARDAHAN', 'Ardahan'), ('ARTVIN', 'Artvin'), ('AYDIN', 'Aydın'), ('BALIKESIR', 'Balıkesir'), ('BARTIN', 'Bartın'), ('BATMAN', 'Batman'), ('BAYBURT', 'Bayburt'), ('BILECIK', 'Bilecik'), ('BINGOL', 'Bingöl'), ('BITLIS', 'Bitlis')]\n",
            "Success: 79 | Failed: 4\n",
            "Saved: iklim_mgm_1991_2020.csv\n"
          ]
        }
      ],
      "source": [
        "!pip -q install requests beautifulsoup4 lxml\n",
        "\n",
        "import re, time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, parse_qs\n",
        "\n",
        "BASE = \"https://www.mgm.gov.tr/veridegerlendirme/il-ve-ilceler-istatistik.aspx\"\n",
        "\n",
        "S = requests.Session()\n",
        "S.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Accept-Language\": \"tr-TR,tr;q=0.9,en-US;q=0.7,en;q=0.6\",\n",
        "    \"Referer\": \"https://www.mgm.gov.tr/\",\n",
        "})\n",
        "\n",
        "NUM_RE = re.compile(r\"[-+]?\\d+(?:[.,]\\d+)?\")\n",
        "\n",
        "def to_float(x):\n",
        "    \"\"\"Convert a string number that may use comma decimal into float.\"\"\"\n",
        "    return float(str(x).strip().replace(\",\", \".\"))\n",
        "\n",
        "def extract_12_from_row(tr):\n",
        "    \"\"\"Extract 12 numeric values from a single <tr> row (months).\"\"\"\n",
        "    cells = [c.get_text(\" \", strip=True) for c in tr.find_all([\"th\", \"td\"])]\n",
        "    nums = []\n",
        "    for c in cells:\n",
        "        found = NUM_RE.findall(c)\n",
        "        for f in found:\n",
        "            try:\n",
        "                nums.append(to_float(f))\n",
        "            except:\n",
        "                pass\n",
        "    return nums[:12] if len(nums) >= 12 else None\n",
        "\n",
        "def find_row_12vals(soup, label_keywords):\n",
        "    \"\"\"\n",
        "    Find the table row that contains all label_keywords and return its 12 monthly values.\n",
        "    label_keywords example:\n",
        "      [\"Ortalama Sıcaklık\"]\n",
        "      [\"Aylık Toplam Yağış Miktarı\", \"Ortalaması\"]\n",
        "    \"\"\"\n",
        "    for tr in soup.find_all(\"tr\"):\n",
        "        row_text = tr.get_text(\" \", strip=True)\n",
        "        if all(k in row_text for k in label_keywords):\n",
        "            vals = extract_12_from_row(tr)\n",
        "            if vals is not None:\n",
        "                return vals\n",
        "    return None\n",
        "\n",
        "def get_cities_from_ankara_page():\n",
        "    \"\"\"\n",
        "    Collect province links by scanning the ANKARA page for anchors that contain 'm='.\n",
        "    Returns: [(m_param, display_name), ...]\n",
        "    \"\"\"\n",
        "    r = S.get(BASE, params={\"k\": \"H\", \"m\": \"ANKARA\"}, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "    cities = []\n",
        "    seen = set()\n",
        "\n",
        "    # Scan all links; extract m= parameter\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if \"m=\" not in href:\n",
        "            continue\n",
        "        full = urljoin(\"https://www.mgm.gov.tr\", href)\n",
        "        q = parse_qs(urlparse(full).query)\n",
        "        m = q.get(\"m\", [None])[0]\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        # Visible province name\n",
        "        disp = a.get_text(strip=True)\n",
        "\n",
        "        # Basic filter to avoid unrelated links\n",
        "        if not disp or len(disp) > 25:\n",
        "            continue\n",
        "\n",
        "        key = (m, disp)\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            cities.append(key)\n",
        "\n",
        "    # Fallback: if nothing found, regex-scan the HTML for k=H&m= links\n",
        "    if len(cities) == 0:\n",
        "        html = r.text\n",
        "        ms = re.findall(\n",
        "            r\"il-ve-ilceler-istatistik\\.aspx\\?k=H(?:&amp;|&)m=([A-Za-zÇĞİÖŞÜçğıöşü0-9]+)\",\n",
        "            html\n",
        "        )\n",
        "        ms = list(dict.fromkeys(ms))  # unique preserving order\n",
        "        cities = [(m, m) for m in ms]\n",
        "\n",
        "    return cities\n",
        "\n",
        "def scrape_city(m_param, disp_name=None):\n",
        "    \"\"\"\n",
        "    Scrape one province page:\n",
        "    - 12 monthly average temperatures\n",
        "    - 12 monthly total precipitation averages\n",
        "    plus computed annual metrics.\n",
        "    \"\"\"\n",
        "    r = S.get(BASE, params={\"k\": \"H\", \"m\": m_param}, timeout=30)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"HTTP {r.status_code}\")\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "    temp12 = find_row_12vals(soup, [\"Ortalama Sıcaklık\"])\n",
        "    prec12 = find_row_12vals(soup, [\"Aylık Toplam Yağış Miktarı\", \"Ortalaması\"])\n",
        "\n",
        "    if temp12 is None or prec12 is None:\n",
        "        raise ValueError(\"Could not find the 12-month temperature/precipitation rows in the table.\")\n",
        "\n",
        "    row = {\n",
        "        \"province\": disp_name if disp_name else m_param,\n",
        "        \"m_param\": m_param,\n",
        "        \"temp_annual_mean\": sum(temp12) / 12.0,\n",
        "        \"prec_annual_total\": sum(prec12),\n",
        "    }\n",
        "\n",
        "    for i, v in enumerate(temp12, start=1):\n",
        "        row[f\"temp_m{i:02d}\"] = v\n",
        "\n",
        "    for i, v in enumerate(prec12, start=1):\n",
        "        row[f\"prec_m{i:02d}\"] = v\n",
        "\n",
        "    return row\n",
        "\n",
        "# =========================\n",
        "# RUN FOR ALL PROVINCES\n",
        "# =========================\n",
        "cities = get_cities_from_ankara_page()\n",
        "print(\"Number of province links found:\", len(cities))\n",
        "print(\"First 20:\", cities[:20])\n",
        "\n",
        "if len(cities) == 0:\n",
        "    raise RuntimeError(\"Could not find the province list. The page structure may have changed.\")\n",
        "\n",
        "rows = []\n",
        "failed = []\n",
        "\n",
        "for idx, (m_param, disp) in enumerate(cities, start=1):\n",
        "    try:\n",
        "        rows.append(scrape_city(m_param, disp))\n",
        "    except Exception as e:\n",
        "        failed.append({\"m_param\": m_param, \"display\": disp, \"error\": str(e)})\n",
        "\n",
        "    time.sleep(0.6)  # be polite with rate-limiting\n",
        "\n",
        "print(\"Success:\", len(rows), \"| Failed:\", len(failed))\n",
        "\n",
        "if rows:\n",
        "    out = pd.DataFrame(rows).sort_values(\"province\").reset_index(drop=True)\n",
        "    out.to_csv(\"iklim_mgm_1991_2020.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"Saved: iklim_mgm_1991_2020.csv\")\n",
        "    # --- Preview the generated CSV in the notebook ---\n",
        "   # print(\"\\n=== Preview: iklim_mgm_1991_2020.csv ===\")\n",
        "    #print(\"Shape:\", out.shape)\n",
        "    #display(out.head(10))          # first 10 rows\n",
        "\n",
        "   # print(\"\\nColumns:\", list(out.columns))\n",
        "    #display(out.tail(5))           # last 5 rows (optional)\n",
        "\n",
        "    # Quick sanity checks\n",
        "    #print(\"\\nMissing values (top):\")\n",
        "    #display(out.isna().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "    #print(\"\\nBasic stats:\")\n",
        "    #display(out[[\"temp_annual_mean\", \"prec_annual_total\"]].describe())\n",
        "\n",
        "\n",
        "if failed:\n",
        "    pd.DataFrame(failed).to_csv(\"iklim_failed.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    #print(\"Saved: iklim_failed.csv\")\n",
        "    #display(pd.DataFrame(failed).head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mali\n"
      ],
      "metadata": {
        "id": "T_x3uNJxoA_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "\n",
        "TP_CSV = \"TurkPatent_all_with_other.csv\"\n",
        "FOODS_XLSX = \"yiyecekler.xlsx\"\n",
        "\n",
        "OUT_LIST_CSV  = \"TurkPatent_ALL_kcal_list.csv\"\n",
        "OUT_LIST_XLSX = \"TurkPatent_ALL_kcal_list.xlsx\"\n",
        "OUT_GROUP_SUMMARY = \"kcal_group_summary.csv\"\n",
        "\n",
        "def norm(s):\n",
        "    return str(s).strip().lower()\n",
        "\n",
        "# 1) Read TurkPatent\n",
        "tp = pd.read_csv(TP_CSV, sep=\";\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Read calorie samples from Excel (ALL sheets)\n",
        "wb = openpyxl.load_workbook(FOODS_XLSX, read_only=True, data_only=True)\n",
        "sheet_names = wb.sheetnames\n",
        "\n",
        "samples = []\n",
        "for sh in sheet_names:\n",
        "    df = pd.read_excel(FOODS_XLSX, sheet_name=sh)\n",
        "    if df.shape[1] < 2:\n",
        "        continue\n",
        "\n",
        "    # IMPORTANT: group name = first column HEADER (not the sheet name)\n",
        "    group_name = norm(df.columns[0])\n",
        "\n",
        "    # first two columns: [sample_food, kcal]\n",
        "    sample_col = df.columns[0]\n",
        "    kcal_col   = df.columns[1]\n",
        "\n",
        "    tmp = df[[sample_col, kcal_col]].copy()\n",
        "    tmp.columns = [\"sample_food\", \"kcal_100g\"]\n",
        "\n",
        "    tmp[\"kcal_group\"] = group_name\n",
        "    tmp[\"sample_food\"] = tmp[\"sample_food\"].astype(str).str.strip()\n",
        "    tmp[\"kcal_100g\"] = pd.to_numeric(tmp[\"kcal_100g\"], errors=\"coerce\")\n",
        "\n",
        "    tmp = tmp.dropna(subset=[\"sample_food\", \"kcal_100g\"])\n",
        "    tmp = tmp[tmp[\"sample_food\"].str.lower().ne(\"nan\")]\n",
        "\n",
        "    samples.append(tmp)\n",
        "\n",
        "samples_df = pd.concat(samples, ignore_index=True)\n",
        "\n",
        "print(\"Extracted sample rows:\", len(samples_df))\n",
        "print(\"Unique kcal_group extracted:\", samples_df[\"kcal_group\"].nunique())\n",
        "print(\"kcal_group list:\", sorted(samples_df[\"kcal_group\"].unique()))\n",
        "\n",
        "# 3) Group summary\n",
        "grp = (samples_df.groupby(\"kcal_group\")\n",
        "       .agg(n_samples=(\"kcal_100g\",\"size\"),\n",
        "            mean_kcal=(\"kcal_100g\",\"mean\"),\n",
        "            median_kcal=(\"kcal_100g\",\"median\"),\n",
        "            std_kcal=(\"kcal_100g\",\"std\"))\n",
        "       .reset_index())\n",
        "\n",
        "grp.to_csv(OUT_GROUP_SUMMARY, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# 4) TurkPatent group -> kcal_group mapping\n",
        "tp_to_kcal_group = {\n",
        "    \"Yemekler ve çorbalar\": \"yemekler ve çorbalar\",\n",
        "    \"Yiyecekler için çeşni / lezzet vericiler, soslar ve tuz\": \"yiyecekler için çeşni\",\n",
        "    \"Peynirler\": \"peynir\",\n",
        "    \"Peynirler ve tereyağı dışında kalan süt ürünleri\": \"süt ürünleri\",\n",
        "    \"İşlenmiş ve işlenmemiş meyve ve sebzeler ile mantarlar\": \"sebze ve meyve\",\n",
        "    \"İşlenmiş ve işlenmemiş et ürünleri\": \"işlenmiş et\",\n",
        "    \"Fırıncılık ve pastacılık mamulleri, hamur işleri, tatlılar\": \"hamurişi tatlı\",\n",
        "    \"Dondurmalar ve yenilebilir buzlar\": \"dondurmalar\",\n",
        "    \"Alkolsüz içecekler\": \"soft içecekler\",\n",
        "    \"Bal\": \"bal\",\n",
        "    \"Biralar ve diğer alkollü içkiler\": \"alkoller\",\n",
        "    \"Çikolata, şekerleme ve türevi ürünler\": \"çikolata/şeker\",\n",
        "    \"Diğer ürünler\": \"diğer\",\n",
        "}\n",
        "\n",
        "map_df = pd.DataFrame({\n",
        "    \"Ürün Grubu\": list(tp_to_kcal_group.keys()),\n",
        "    \"kcal_group\": [norm(v) for v in tp_to_kcal_group.values()]\n",
        "})\n",
        "\n",
        "# Attach kcal stats\n",
        "map_df = map_df.merge(grp, on=\"kcal_group\", how=\"left\")\n",
        "\n",
        "# DEBUG: show which mappings didn't find any stats\n",
        "missing = map_df[map_df[\"mean_kcal\"].isna()][[\"Ürün Grubu\",\"kcal_group\"]]\n",
        "if len(missing) > 0:\n",
        "    print(\"\\nThese mapped kcal_group names were NOT found in Excel groups:\")\n",
        "    print(missing.to_string(index=False))\n",
        "    print(\"\\nAvailable Excel kcal_group names:\")\n",
        "    print(sorted(grp[\"kcal_group\"].unique()))\n",
        "\n",
        "# 5) Assign calories to TurkPatent\n",
        "tp2 = tp.merge(\n",
        "    map_df[[\"Ürün Grubu\",\"kcal_group\",\"mean_kcal\",\"median_kcal\",\"n_samples\",\"std_kcal\"]],\n",
        "    on=\"Ürün Grubu\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "tp2 = tp2.rename(columns={\n",
        "    \"Coğrafi İşaretin Adı\": \"food_name\",\n",
        "    \"İl\": \"province\",\n",
        "    \"Ürün Grubu\": \"tp_group\"\n",
        "})\n",
        "\n",
        "tp2[\"kcal_100g\"] = tp2[\"mean_kcal\"]\n",
        "mask = tp2[\"kcal_100g\"].isna()\n",
        "tp2.loc[mask, \"kcal_100g\"] = tp2.loc[mask, \"median_kcal\"]\n",
        "\n",
        "# 6) Export final list\n",
        "final_cols = [\"food_name\",\"province\",\"tp_group\",\"kcal_group\",\"kcal_100g\",\"n_samples\",\"std_kcal\"]\n",
        "final = tp2[final_cols].copy()\n",
        "\n",
        "final.to_csv(OUT_LIST_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "final.to_excel(OUT_LIST_XLSX, index=False)\n",
        "\n",
        "print(\"\\nOutputs created:\")\n",
        "print(\"-\", OUT_LIST_CSV)\n",
        "print(\"-\", OUT_LIST_XLSX)\n",
        "print(\"-\", OUT_GROUP_SUMMARY)\n",
        "print(\"Total rows:\", len(final))\n",
        "print(\"Rows with kcal:\", final[\"kcal_100g\"].notna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moiakZiCoJlE",
        "outputId": "d9b7b22c-b9c2-4189-a088-9fb8528c0e7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted sample rows: 131\n",
            "Unique kcal_group extracted: 14\n",
            "kcal_group list: ['alkoller', 'bal', 'diğer', 'dondurmalar', 'hamurişi tatlı', 'işlenmiş et', 'peynir', 'sebze ve meyve', 'soft içecekler', 'süt ürünleri', 'tereyağı ve sıvıyağlar', 'yemekler ve çorbalar', 'yiyecekler için çeşni', 'çikolata/şeker']\n",
            "\n",
            "Outputs created:\n",
            "- TurkPatent_ALL_kcal_list.csv\n",
            "- TurkPatent_ALL_kcal_list.xlsx\n",
            "- kcal_group_summary.csv\n",
            "Total rows: 1514\n",
            "Rows with kcal: 1514\n"
          ]
        }
      ]
    }
  ]
}