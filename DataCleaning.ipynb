{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZWOVgEpI3HFKJjWwWAxvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maliozdemir1/dsa210-project-impact-of-climate-on-gastronomic-diversity/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection & Cleaning"
      ],
      "metadata": {
        "id": "TAIKi-ERrpPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = \"https://www.mgm.gov.tr/veridegerlendirme/il-ve-ilceler-istatistik.aspx\"\n",
        "\n",
        "S = requests.Session()\n",
        "S.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Accept-Language\": \"tr-TR,tr;q=0.9,en-US;q=0.7,en;q=0.6\",\n",
        "    \"Referer\": \"https://www.mgm.gov.tr/\",\n",
        "})\n",
        "\n",
        "NUM_RE = re.compile(r\"[-+]?\\d+(?:[.,]\\d+)?\")\n",
        "\n",
        "def to_float(x):\n",
        "    \"\"\"Convert a string number that may use comma decimal into float.\"\"\"\n",
        "    return float(str(x).strip().replace(\",\", \".\"))\n",
        "\n",
        "def extract_12_from_row(tr):\n",
        "    \"\"\"Extract 12 numeric values from a single <tr> row (months).\"\"\"\n",
        "    cells = [c.get_text(\" \", strip=True) for c in tr.find_all([\"th\", \"td\"])]\n",
        "    nums = []\n",
        "    for c in cells:\n",
        "        found = NUM_RE.findall(c)\n",
        "        for f in found:\n",
        "            try:\n",
        "                nums.append(to_float(f))\n",
        "            except:\n",
        "                pass\n",
        "    return nums[:12] if len(nums) >= 12 else None\n",
        "\n",
        "def find_row_12vals(soup, label_keywords):\n",
        "    \"\"\"\n",
        "    Find the table row that contains all label_keywords and return its 12 monthly values.\n",
        "    label_keywords example:\n",
        "      [\"Ortalama Sıcaklık\"]\n",
        "      [\"Aylık Toplam Yağış Miktarı\", \"Ortalaması\"]\n",
        "    \"\"\"\n",
        "    for tr in soup.find_all(\"tr\"):\n",
        "        row_text = tr.get_text(\" \", strip=True)\n",
        "        if all(k in row_text for k in label_keywords):\n",
        "            vals = extract_12_from_row(tr)\n",
        "            if vals is not None:\n",
        "                return vals\n",
        "    return None\n",
        "\n",
        "def get_cities_from_ankara_page():\n",
        "    \"\"\"\n",
        "    Collect province links by scanning the ANKARA page for anchors that contain 'm='.\n",
        "    Returns: [(m_param, display_name), ...]\n",
        "    \"\"\"\n",
        "    r = S.get(BASE, params={\"k\": \"H\", \"m\": \"ANKARA\"}, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "    cities = []\n",
        "    seen = set()\n",
        "\n",
        "    # Scan all links; extract m= parameter\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if \"m=\" not in href:\n",
        "            continue\n",
        "        full = urljoin(\"https://www.mgm.gov.tr\", href)\n",
        "        q = parse_qs(urlparse(full).query)\n",
        "        m = q.get(\"m\", [None])[0]\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        # Visible province name\n",
        "        disp = a.get_text(strip=True)\n",
        "\n",
        "        # Basic filter to avoid unrelated links\n",
        "        if not disp or len(disp) > 25:\n",
        "            continue\n",
        "\n",
        "        key = (m, disp)\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            cities.append(key)\n",
        "\n",
        "    # Fallback: if nothing found, regex-scan the HTML for k=H&m= links\n",
        "    if len(cities) == 0:\n",
        "        html = r.text\n",
        "        ms = re.findall(\n",
        "            r\"il-ve-ilceler-istatistik\\.aspx\\?k=H(?:&amp;|&)m=([A-Za-zÇĞİÖŞÜçğıöşü0-9]+)\",\n",
        "            html\n",
        "        )\n",
        "        ms = list(dict.fromkeys(ms))  # unique preserving order\n",
        "        cities = [(m, m) for m in ms]\n",
        "\n",
        "    return cities\n",
        "\n",
        "def scrape_city(m_param, disp_name=None):\n",
        "    \"\"\"\n",
        "    Scrape one province page:\n",
        "    - 12 monthly average temperatures\n",
        "    - 12 monthly total precipitation averages\n",
        "    plus computed annual metrics.\n",
        "    \"\"\"\n",
        "    r = S.get(BASE, params={\"k\": \"H\", \"m\": m_param}, timeout=30)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"HTTP {r.status_code}\")\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "    temp12 = find_row_12vals(soup, [\"Ortalama Sıcaklık\"])\n",
        "    prec12 = find_row_12vals(soup, [\"Aylık Toplam Yağış Miktarı\", \"Ortalaması\"])\n",
        "\n",
        "    if temp12 is None or prec12 is None:\n",
        "        raise ValueError(\"Could not find the 12-month temperature/precipitation rows in the table.\")\n",
        "\n",
        "    row = {\n",
        "        \"province\": disp_name if disp_name else m_param,\n",
        "        \"m_param\": m_param,\n",
        "        \"temp_annual_mean\": sum(temp12) / 12.0,\n",
        "        \"prec_annual_total\": sum(prec12),\n",
        "    }\n",
        "\n",
        "    for i, v in enumerate(temp12, start=1):\n",
        "        row[f\"temp_m{i:02d}\"] = v\n",
        "\n",
        "    for i, v in enumerate(prec12, start=1):\n",
        "        row[f\"prec_m{i:02d}\"] = v\n",
        "\n",
        "    return row\n",
        "\n",
        "# =========================\n",
        "# RUN FOR ALL PROVINCES\n",
        "# =========================\n",
        "cities = get_cities_from_ankara_page()\n",
        "print(\"Number of province links found:\", len(cities))\n",
        "print(\"First 20:\", cities[:20])\n",
        "\n",
        "if len(cities) == 0:\n",
        "    raise RuntimeError(\"Could not find the province list. The page structure may have changed.\")\n",
        "\n",
        "rows = []\n",
        "failed = []\n",
        "\n",
        "for idx, (m_param, disp) in enumerate(cities, start=1):\n",
        "    try:\n",
        "        rows.append(scrape_city(m_param, disp))\n",
        "    except Exception as e:\n",
        "        failed.append({\"m_param\": m_param, \"display\": disp, \"error\": str(e)})\n",
        "\n",
        "    time.sleep(0.6)  # be polite with rate-limiting\n",
        "\n",
        "print(\"Success:\", len(rows), \"| Failed:\", len(failed))\n",
        "\n",
        "if rows:\n",
        "    out = pd.DataFrame(rows).sort_values(\"province\").reset_index(drop=True)\n",
        "    out.to_csv(\"iklim_mgm_1991_2020.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"Saved: iklim_mgm_1991_2020.csv\")\n",
        "    # --- Preview the generated CSV in the notebook ---\n",
        "   # print(\"\\n=== Preview: iklim_mgm_1991_2020.csv ===\")\n",
        "    #print(\"Shape:\", out.shape)\n",
        "    #display(out.head(10))          # first 10 rows\n",
        "\n",
        "   # print(\"\\nColumns:\", list(out.columns))\n",
        "    #display(out.tail(5))           # last 5 rows (optional)\n",
        "\n",
        "    # Quick sanity checks\n",
        "    #print(\"\\nMissing values (top):\")\n",
        "    #display(out.isna().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "    #print(\"\\nBasic stats:\")\n",
        "    #display(out[[\"temp_annual_mean\", \"prec_annual_total\"]].describe())\n",
        "\n",
        "\n",
        "if failed:\n",
        "    pd.DataFrame(failed).to_csv(\"iklim_failed.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    #print(\"Saved: iklim_failed.csv\")\n",
        "    #display(pd.DataFrame(failed).head(15))\n"
      ],
      "metadata": {
        "id": "0_kxqHP7yTQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is for web scraping of province-level climate data from MGM's website, by automatically visiting each page of a province and getting the monthly average temperature and monthly total precipitation averages of the 1991-2020 period.\n",
        "\n",
        "By using these monthly values, annual mean temperature and annual total precipitation are calculated. Then, the final dataset is saved as a CSV file, whereas provinces that fail during scraping are logged separately."
      ],
      "metadata": {
        "id": "F_8mCwb_yWNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "\n",
        "TP_CSV = \"TurkPatent_all_with_other.csv\"\n",
        "FOODS_XLSX = \"yiyecekler.xlsx\"\n",
        "\n",
        "OUT_LIST_CSV  = \"TurkPatent_ALL_kcal_list.csv\"\n",
        "OUT_LIST_XLSX = \"TurkPatent_ALL_kcal_list.xlsx\"\n",
        "OUT_GROUP_SUMMARY = \"kcal_group_summary.csv\"\n",
        "\n",
        "def norm(s):\n",
        "    return str(s).strip().lower()\n",
        "\n",
        "# 1) Read TurkPatent\n",
        "tp = pd.read_csv(TP_CSV, sep=\";\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Read calorie samples from Excel (ALL sheets)\n",
        "wb = openpyxl.load_workbook(FOODS_XLSX, read_only=True, data_only=True)\n",
        "sheet_names = wb.sheetnames\n",
        "\n",
        "samples = []\n",
        "for sh in sheet_names:\n",
        "    df = pd.read_excel(FOODS_XLSX, sheet_name=sh)\n",
        "    if df.shape[1] < 2:\n",
        "        continue\n",
        "\n",
        "    # IMPORTANT: group name = first column HEADER (not the sheet name)\n",
        "    group_name = norm(df.columns[0])\n",
        "\n",
        "    # first two columns: [sample_food, kcal]\n",
        "    sample_col = df.columns[0]\n",
        "    kcal_col   = df.columns[1]\n",
        "\n",
        "    tmp = df[[sample_col, kcal_col]].copy()\n",
        "    tmp.columns = [\"sample_food\", \"kcal_100g\"]\n",
        "\n",
        "    tmp[\"kcal_group\"] = group_name\n",
        "    tmp[\"sample_food\"] = tmp[\"sample_food\"].astype(str).str.strip()\n",
        "    tmp[\"kcal_100g\"] = pd.to_numeric(tmp[\"kcal_100g\"], errors=\"coerce\")\n",
        "\n",
        "    tmp = tmp.dropna(subset=[\"sample_food\", \"kcal_100g\"])\n",
        "    tmp = tmp[tmp[\"sample_food\"].str.lower().ne(\"nan\")]\n",
        "\n",
        "    samples.append(tmp)\n",
        "\n",
        "samples_df = pd.concat(samples, ignore_index=True)\n",
        "\n",
        "print(\"Extracted sample rows:\", len(samples_df))\n",
        "print(\"Unique kcal_group extracted:\", samples_df[\"kcal_group\"].nunique())\n",
        "print(\"kcal_group list:\", sorted(samples_df[\"kcal_group\"].unique()))\n",
        "\n",
        "# 3) Group summary\n",
        "grp = (samples_df.groupby(\"kcal_group\")\n",
        "       .agg(n_samples=(\"kcal_100g\",\"size\"),\n",
        "            mean_kcal=(\"kcal_100g\",\"mean\"),\n",
        "            median_kcal=(\"kcal_100g\",\"median\"),\n",
        "            std_kcal=(\"kcal_100g\",\"std\"))\n",
        "       .reset_index())\n",
        "\n",
        "grp.to_csv(OUT_GROUP_SUMMARY, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# 4) TurkPatent group -> kcal_group mapping\n",
        "tp_to_kcal_group = {\n",
        "    \"Yemekler ve çorbalar\": \"yemekler ve çorbalar\",\n",
        "    \"Yiyecekler için çeşni / lezzet vericiler, soslar ve tuz\": \"yiyecekler için çeşni\",\n",
        "    \"Peynirler\": \"peynir\",\n",
        "    \"Peynirler ve tereyağı dışında kalan süt ürünleri\": \"süt ürünleri\",\n",
        "    \"İşlenmiş ve işlenmemiş meyve ve sebzeler ile mantarlar\": \"sebze ve meyve\",\n",
        "    \"İşlenmiş ve işlenmemiş et ürünleri\": \"işlenmiş et\",\n",
        "    \"Fırıncılık ve pastacılık mamulleri, hamur işleri, tatlılar\": \"hamurişi tatlı\",\n",
        "    \"Dondurmalar ve yenilebilir buzlar\": \"dondurmalar\",\n",
        "    \"Alkolsüz içecekler\": \"soft içecekler\",\n",
        "    \"Bal\": \"bal\",\n",
        "    \"Biralar ve diğer alkollü içkiler\": \"alkoller\",\n",
        "    \"Çikolata, şekerleme ve türevi ürünler\": \"çikolata/şeker\",\n",
        "    \"Diğer ürünler\": \"diğer\",\n",
        "}\n",
        "\n",
        "map_df = pd.DataFrame({\n",
        "    \"Ürün Grubu\": list(tp_to_kcal_group.keys()),\n",
        "    \"kcal_group\": [norm(v) for v in tp_to_kcal_group.values()]\n",
        "})\n",
        "\n",
        "# Attach kcal stats\n",
        "map_df = map_df.merge(grp, on=\"kcal_group\", how=\"left\")\n",
        "\n",
        "# DEBUG: show which mappings didn't find any stats\n",
        "missing = map_df[map_df[\"mean_kcal\"].isna()][[\"Ürün Grubu\",\"kcal_group\"]]\n",
        "if len(missing) > 0:\n",
        "    print(\"\\nThese mapped kcal_group names were NOT found in Excel groups:\")\n",
        "    print(missing.to_string(index=False))\n",
        "    print(\"\\nAvailable Excel kcal_group names:\")\n",
        "    print(sorted(grp[\"kcal_group\"].unique()))\n",
        "\n",
        "# 5) Assign calories to TurkPatent\n",
        "tp2 = tp.merge(\n",
        "    map_df[[\"Ürün Grubu\",\"kcal_group\",\"mean_kcal\",\"median_kcal\",\"n_samples\",\"std_kcal\"]],\n",
        "    on=\"Ürün Grubu\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "tp2 = tp2.rename(columns={\n",
        "    \"Coğrafi İşaretin Adı\": \"food_name\",\n",
        "    \"İl\": \"province\",\n",
        "    \"Ürün Grubu\": \"tp_group\"\n",
        "})\n",
        "\n",
        "tp2[\"kcal_100g\"] = tp2[\"mean_kcal\"]\n",
        "mask = tp2[\"kcal_100g\"].isna()\n",
        "tp2.loc[mask, \"kcal_100g\"] = tp2.loc[mask, \"median_kcal\"]\n",
        "\n",
        "# 6) Export final list\n",
        "final_cols = [\"food_name\",\"province\",\"tp_group\",\"kcal_group\",\"kcal_100g\",\"n_samples\",\"std_kcal\"]\n",
        "final = tp2[final_cols].copy()\n",
        "\n",
        "final.to_csv(OUT_LIST_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "final.to_excel(OUT_LIST_XLSX, index=False)\n",
        "\n",
        "print(\"\\nOutputs created:\")\n",
        "print(\"-\", OUT_LIST_CSV)\n",
        "print(\"-\", OUT_LIST_XLSX)\n",
        "print(\"-\", OUT_GROUP_SUMMARY)\n",
        "print(\"Total rows:\", len(final))\n",
        "print(\"Rows with kcal:\", final[\"kcal_100g\"].notna().sum())\n"
      ],
      "metadata": {
        "id": "r6g5w8S5tc6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides an estimated value for kcal/100g for any food from the TurkPatent GI database. The estimated value is arrived at by utilizing calorie samples from an Excel file which contains calories of the food. The script will read all sheets from the Excel workbook, create groups of calorie samples (based on the First column name from each sheet), generate statistics for calorie groups (mean/median/std), match TurkPatent groups with the groups of calorie samples, append the statistics to the TurkPatent dataframe, and finally generate a list of estimated food-level kcal."
      ],
      "metadata": {
        "id": "dhu3GZK2tdYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ==========\n",
        "# INPUT / OUTPUT\n",
        "# ==========\n",
        "MAPPED_PATH = \"TurkPatent_ALL_kcal_list.csv\"   # your mean-mapped output\n",
        "OUT_MAPPED  = \"mean_mapped_clean_en.csv\"\n",
        "\n",
        "# ==========\n",
        "# Helpers\n",
        "# ==========\n",
        "TR_LOWER_MAP = str.maketrans({\"I\":\"ı\",\"İ\":\"i\"})\n",
        "TR_UPPER_MAP = str.maketrans({\"i\":\"İ\",\"ı\":\"I\"})\n",
        "\n",
        "def tr_title(s: str):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "    if not s:\n",
        "        return s\n",
        "    s_low = s.translate(TR_LOWER_MAP).lower()\n",
        "    parts = []\n",
        "    for w in s_low.split(\" \"):\n",
        "        if not w:\n",
        "            continue\n",
        "        parts.append(w[0].translate(TR_UPPER_MAP).upper() + w[1:])\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def clean_text(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "# Optional: TR -> EN group names (nice for reporting)\n",
        "GROUP_TR_TO_EN = {\n",
        "    \"Yemekler ve çorbalar\": \"Meals & soups\",\n",
        "    \"Yiyecekler için çeşni / lezzet vericiler, soslar ve tuz\": \"Seasonings, sauces & salt\",\n",
        "    \"Peynirler\": \"Cheese\",\n",
        "    \"Peynirler ve tereyağı dışında kalan süt ürünleri\": \"Dairy (excluding cheese & butter)\",\n",
        "    \"İşlenmiş ve işlenmemiş meyve ve sebzeler ile mantarlar\": \"Fruits, vegetables & mushrooms\",\n",
        "    \"İşlenmiş ve işlenmemiş et ürünleri\": \"Meat products\",\n",
        "    \"Fırıncılık ve pastacılık mamulleri, hamur işleri, tatlılar\": \"Bakery, pastry & desserts\",\n",
        "    \"Dondurmalar ve yenilebilir buzlar\": \"Ice cream & edible ice\",\n",
        "    \"Alkolsüz içecekler\": \"Non-alcoholic beverages\",\n",
        "    \"Bal\": \"Honey\",\n",
        "    \"Biralar ve diğer alkollü içkiler\": \"Alcoholic beverages\",\n",
        "    \"Çikolata, şekerleme ve türevi ürünler\": \"Chocolate & confectionery\",\n",
        "    \"Diğer ürünler\": \"Other products\",\n",
        "}\n",
        "\n",
        "# ==========\n",
        "# Load & rename\n",
        "# ==========\n",
        "mapped = pd.read_csv(MAPPED_PATH, encoding=\"utf-8-sig\")\n",
        "\n",
        "# Expect columns: food_name, province, tp_group, kcal_100g, ...\n",
        "mapped_clean = mapped.rename(columns={\n",
        "    \"tp_group\": \"product_group_tr\",\n",
        "}).copy()\n",
        "\n",
        "# ==========\n",
        "# Clean text + types\n",
        "# ==========\n",
        "mapped_clean[\"province\"] = mapped_clean[\"province\"].apply(tr_title)\n",
        "mapped_clean[\"food_name\"] = mapped_clean[\"food_name\"].apply(clean_text)\n",
        "mapped_clean[\"product_group_tr\"] = mapped_clean[\"product_group_tr\"].apply(clean_text)\n",
        "mapped_clean[\"kcal_100g\"] = pd.to_numeric(mapped_clean[\"kcal_100g\"], errors=\"coerce\")\n",
        "\n",
        "mapped_clean[\"product_group_en\"] = mapped_clean[\"product_group_tr\"].map(GROUP_TR_TO_EN).fillna(\"Unknown\")\n",
        "\n",
        "# Keep only columns you need\n",
        "mapped_clean = mapped_clean[[\"province\",\"food_name\",\"product_group_tr\",\"product_group_en\",\"kcal_100g\"]].copy()\n",
        "\n",
        "# Drop empty essentials, remove duplicates\n",
        "mapped_clean = mapped_clean.dropna(subset=[\"province\",\"food_name\",\"product_group_tr\"])\n",
        "mapped_clean = mapped_clean.drop_duplicates(subset=[\"province\",\"food_name\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "# ==========\n",
        "# Save\n",
        "# ==========\n",
        "mapped_clean.to_csv(OUT_MAPPED, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Saved:\", OUT_MAPPED)\n",
        "#print(\"Rows:\", len(mapped_clean))\n",
        "#display(mapped_clean.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "5ldASfyOurNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this code, we have gathered the kcal-mapped TurkPatent dataset, cleaned text formatting (Turkish-aware province casing), added English group labels, removed duplicates, and exported a cleaner CSV for analysis/reporting."
      ],
      "metadata": {
        "id": "_kGWpenJt4ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"mean_mapped_clean_en.csv\", encoding=\"utf-8-sig\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "sVxvmEvI1E2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "mapped_clean = pd.read_csv(\"mean_mapped_clean_en.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "summary = (mapped_clean.groupby(\"product_group_en\")[\"kcal_100g\"]\n",
        "           .agg(\n",
        "               n_foods=\"size\",\n",
        "               n_unique=\"nunique\",\n",
        "               mean_kcal=\"mean\",\n",
        "               min_kcal=\"min\",\n",
        "               max_kcal=\"max\"\n",
        "           )\n",
        "           .reset_index()\n",
        "           .sort_values(\"mean_kcal\", ascending=False))\n",
        "\n",
        "print(\"Mean-mapped group calories (should be constant within each group):\")\n",
        "display(summary.head(30))\n"
      ],
      "metadata": {
        "id": "lNKoK6zbyOhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proof = summary.copy()\n",
        "proof[\"is_constant_within_group\"] = (proof[\"n_unique\"] == 1) & (proof[\"min_kcal\"] == proof[\"max_kcal\"])\n",
        "display(proof[[\"product_group_en\",\"n_foods\",\"n_unique\",\"mean_kcal\",\"min_kcal\",\"max_kcal\",\"is_constant_within_group\"]])\n"
      ],
      "metadata": {
        "id": "K675Co9nzEHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grp_std = (mapped_clean.groupby(\"product_group_en\")[\"kcal_100g\"]\n",
        "           .agg(n=\"size\", std=\"std\", mean=\"mean\")\n",
        "           .sort_values(\"std\"))\n",
        "\n",
        "print(\"Within-group std (mean-mapped):\")\n",
        "display(grp_std.reset_index())\n"
      ],
      "metadata": {
        "id": "FquAFGAWzPM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_groups = mapped_clean[\"product_group_en\"].value_counts().head(10).index.tolist()\n",
        "data = [mapped_clean.loc[mapped_clean[\"product_group_en\"]==g, \"kcal_100g\"].dropna().values for g in top_groups]\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.boxplot(data, labels=top_groups, showfliers=False)\n",
        "plt.title(\"Mean-mapped calories by product group (expect near-constant values)\")\n",
        "plt.ylabel(\"kcal/100g\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bqgSxCYJzPKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "MANUAL_PATH = \"kaloriler_manual.csv\"\n",
        "OUT_MANUAL  = \"manual_clean_en.csv\"\n",
        "\n",
        "TR_LOWER_MAP = str.maketrans({\"I\":\"ı\",\"İ\":\"i\"})\n",
        "TR_UPPER_MAP = str.maketrans({\"i\":\"İ\",\"ı\":\"I\"})\n",
        "\n",
        "def tr_title(s: str):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "    if not s:\n",
        "        return s\n",
        "    s_low = s.translate(TR_LOWER_MAP).lower()\n",
        "    parts = []\n",
        "    for w in s_low.split(\" \"):\n",
        "        if not w:\n",
        "            continue\n",
        "        parts.append(w[0].translate(TR_UPPER_MAP).upper() + w[1:])\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def clean_text(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "GROUP_TR_TO_EN = {\n",
        "    \"Yemekler ve çorbalar\": \"Meals & soups\",\n",
        "    \"Yiyecekler için çeşni / lezzet vericiler, soslar ve tuz\": \"Seasonings, sauces & salt\",\n",
        "    \"Peynirler\": \"Cheese\",\n",
        "    \"Peynirler ve tereyağı dışında kalan süt ürünleri\": \"Dairy (excluding cheese & butter)\",\n",
        "    \"İşlenmiş ve işlenmemiş meyve ve sebzeler ile mantarlar\": \"Fruits, vegetables & mushrooms\",\n",
        "    \"İşlenmiş ve işlenmemiş et ürünleri\": \"Meat products\",\n",
        "    \"Fırıncılık ve pastacılık mamulleri, hamur işleri, tatlılar\": \"Bakery, pastry & desserts\",\n",
        "    \"Dondurmalar ve yenilebilir buzlar\": \"Ice cream & edible ice\",\n",
        "    \"Alkolsüz içecekler\": \"Non-alcoholic beverages\",\n",
        "    \"Bal\": \"Honey\",\n",
        "    \"Biralar ve diğer alkollü içkiler\": \"Alcoholic beverages\",\n",
        "    \"Çikolata, şekerleme ve türevi ürünler\": \"Chocolate & confectionery\",\n",
        "    \"Diğer ürünler\": \"Other products\",\n",
        "}\n",
        "\n",
        "manual = pd.read_csv(MANUAL_PATH, encoding=\"utf-8-sig\")\n",
        "\n",
        "manual_clean = manual.rename(columns={\n",
        "    \"İl\": \"province\",\n",
        "    \"Coğrafi İşaretin Adı\": \"food_name\",\n",
        "    \"Ürün Grubu\": \"product_group_tr\",\n",
        "    \"Kalori\": \"kcal_100g\"\n",
        "}).copy()\n",
        "\n",
        "manual_clean[\"province\"] = manual_clean[\"province\"].apply(tr_title)\n",
        "manual_clean[\"food_name\"] = manual_clean[\"food_name\"].apply(clean_text)\n",
        "manual_clean[\"product_group_tr\"] = manual_clean[\"product_group_tr\"].apply(clean_text)\n",
        "manual_clean[\"kcal_100g\"] = pd.to_numeric(manual_clean[\"kcal_100g\"], errors=\"coerce\")\n",
        "\n",
        "manual_clean[\"product_group_en\"] = manual_clean[\"product_group_tr\"].map(GROUP_TR_TO_EN).fillna(\"Unknown\")\n",
        "\n",
        "manual_clean = manual_clean.dropna(subset=[\"province\",\"food_name\",\"product_group_tr\",\"kcal_100g\"])\n",
        "manual_clean = manual_clean.drop_duplicates(subset=[\"province\",\"food_name\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "manual_clean = manual_clean[[\"province\",\"food_name\",\"product_group_tr\",\"product_group_en\",\"kcal_100g\"]].copy()\n",
        "manual_clean.to_csv(OUT_MANUAL, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Saved:\", OUT_MANUAL)\n",
        "print(\"Rows:\", len(manual_clean))\n",
        "display(manual_clean.head(10))\n"
      ],
      "metadata": {
        "id": "pknKiVds0TqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In here, calorie information that is gathered manually is processed. It removes text information, resolves province names in Turkish, processes calorie information into numeric format, and removes duplicate the incomplete records. The product groups are also given English labels to make it easier to use these data for in other steps."
      ],
      "metadata": {
        "id": "g03Pp1ycuoGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1) Read the cleaned output\n",
        "df = pd.read_csv(\"manual_clean_en.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Cleaned dataset loaded successfully.\")\n",
        "print(\"Shape (rows, columns):\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# 2) Preview\n",
        "display(df.head(10))\n",
        "\n",
        "# 3) Quick data quality checks\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isna().sum().to_frame(\"missing\"))\n",
        "\n",
        "print(\"\\nTop 10 English product groups (counts):\")\n",
        "display(df[\"product_group_en\"].value_counts().head(10).to_frame(\"count\"))\n",
        "\n",
        "print(\"\\nBasic kcal statistics:\")\n",
        "display(df[\"kcal_100g\"].describe().to_frame().T)\n"
      ],
      "metadata": {
        "id": "zM1xEXEFGx46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have controlled the missing values, and see the output of our data."
      ],
      "metadata": {
        "id": "33O7gXFuw_xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# TR -> EN category mapping (TurkPatent product groups)\n",
        "GROUP_TR_TO_EN = {\n",
        "    \"Yemekler ve çorbalar\": \"Meals & soups\",\n",
        "    \"Yiyecekler için çeşni / lezzet vericiler, soslar ve tuz\": \"Seasonings, sauces & salt\",\n",
        "    \"Peynirler\": \"Cheese\",\n",
        "    \"Peynirler ve tereyağı dışında kalan süt ürünleri\": \"Dairy (excluding cheese & butter)\",\n",
        "    \"İşlenmiş ve işlenmemiş meyve ve sebzeler ile mantarlar\": \"Fruits, vegetables & mushrooms\",\n",
        "    \"İşlenmiş ve işlenmemiş et ürünleri\": \"Meat products\",\n",
        "    \"Fırıncılık ve pastacılık mamulleri, hamur işleri, tatlılar\": \"Bakery, pastry & desserts\",\n",
        "    \"Dondurmalar ve yenilebilir buzlar\": \"Ice cream & edible ice\",\n",
        "    \"Alkolsüz içecekler\": \"Non-alcoholic beverages\",\n",
        "    \"Bal\": \"Honey\",\n",
        "    \"Biralar ve diğer alkollü içkiler\": \"Alcoholic beverages\",\n",
        "    \"Çikolata, şekerleme ve türevi ürünler\": \"Chocolate & confectionery\",\n",
        "    \"Diğer ürünler\": \"Other products\",\n",
        "}\n",
        "\n",
        "def clean_turkpatent_gi_data(\n",
        "    input_csv=\"türkpatent tescilli ürünler ham.csv\",\n",
        "    output_csv=\"TurkPatent_clean_en.csv\",\n",
        "    sep=\";\"\n",
        "):\n",
        "    # 1) Load\n",
        "    df = pd.read_csv(input_csv, sep=sep, encoding=\"utf-8-sig\")\n",
        "\n",
        "    required = [\"İl\", \"Coğrafi Işaretin Adı\", \"Ürün grubu\", \"Başvuru Yapan/Tescil Ettiren\"]\n",
        "    missing_cols = [c for c in required if c not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns in input file: {missing_cols}\")\n",
        "\n",
        "    # 2) Basic string cleanup\n",
        "    for c in required:\n",
        "        df[c] = df[c].astype(str).replace(\"nan\", np.nan).replace(\"None\", np.nan)\n",
        "        df[c] = df[c].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip() if isinstance(x, str) else x)\n",
        "\n",
        "    # 3) Fill missing provinces (case-insensitive)\n",
        "    known_cities = df[\"İl\"].dropna().unique().tolist()\n",
        "    known_cities = [re.sub(r\"\\s+\", \" \", c).strip() for c in known_cities if str(c).strip()]\n",
        "    known_cities_lower = [(c, c.lower()) for c in known_cities]\n",
        "\n",
        "    def fill_city(row):\n",
        "        if pd.notna(row[\"İl\"]) and str(row[\"İl\"]).strip():\n",
        "            return row[\"İl\"]\n",
        "\n",
        "        text = f\"{row.get('Coğrafi Işaretin Adı','')} {row.get('Başvuru Yapan/Tescil Ettiren','')}\"\n",
        "        text_l = str(text).lower()\n",
        "\n",
        "        for original, city_l in known_cities_lower:\n",
        "            if re.search(rf\"(?<!\\w){re.escape(city_l)}(?!\\w)\", text_l):\n",
        "                return original\n",
        "        return np.nan\n",
        "\n",
        "    before_missing = df[\"İl\"].isna().sum()\n",
        "    df[\"İl\"] = df.apply(fill_city, axis=1)\n",
        "    after_missing = df[\"İl\"].isna().sum()\n",
        "\n",
        "    # 4) Select needed columns + rename to English\n",
        "    target = df[[\"Coğrafi Işaretin Adı\", \"Ürün grubu\", \"İl\"]].copy()\n",
        "    target.columns = [\"food_name\", \"product_group_tr\", \"province\"]\n",
        "\n",
        "    # Final trim\n",
        "    for c in [\"food_name\", \"product_group_tr\", \"province\"]:\n",
        "        target[c] = target[c].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip() if pd.notna(x) else x)\n",
        "\n",
        "    target = target.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
        "    target = target.dropna(subset=[\"food_name\", \"product_group_tr\"]).reset_index(drop=True)\n",
        "\n",
        "    # 5) Create English category column\n",
        "    target[\"product_group_en\"] = target[\"product_group_tr\"].map(GROUP_TR_TO_EN).fillna(\"Unknown\")\n",
        "\n",
        "    # 6) Remove duplicates\n",
        "    target = target.drop_duplicates(subset=[\"province\", \"food_name\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "    # 7) Save\n",
        "    target.to_csv(output_csv, sep=sep, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # 8) English summary\n",
        "    print(\"Cleaning completed.\")\n",
        "    print(\"Input rows:\", len(df))\n",
        "    print(\"Output rows:\", len(target))\n",
        "    print(\"Missing provinces (before):\", before_missing)\n",
        "    print(\"Missing provinces (after):\", after_missing)\n",
        "    print(\"Filled provinces:\", before_missing - after_missing)\n",
        "    print(\"Saved file:\", output_csv)\n",
        "\n",
        "    return target\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_turkpatent_gi_data()\n"
      ],
      "metadata": {
        "id": "e5eCc5ib_hnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "out = pd.read_csv(\"TurkPatent_clean_en.csv\", sep=\";\", encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(\"Shape (rows, columns):\", out.shape)\n",
        "print(\"Columns:\", list(out.columns))\n",
        "\n",
        "# show a preview with English group labels\n",
        "display(out[[\"province\", \"food_name\", \"product_group_en\"]].head(10))\n",
        "\n",
        "print(\"\\nTop 10 English product groups (counts):\")\n",
        "display(out[\"product_group_en\"].value_counts().head(10).to_frame(\"count\"))\n"
      ],
      "metadata": {
        "id": "QNkEJ04VBRPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we got the final data which will be used in our analysis methods."
      ],
      "metadata": {
        "id": "Wo_ZiQAbxS42"
      }
    }
  ]
}